# AWS Heart Disease Data Pipeline & Analytics

An end-to-end **big data analytics pipeline** built on **AWS** to analyze and visualize heart disease risk factors using large-scale survey data.  
This project demonstrates **data lake architecture**, **ETL with AWS Glue**, **distributed processing with Apache Spark**, and **analytics-driven visualization**.

---

## ğŸ“Œ Project Goal

To evaluate how effectively survey responses can be used to:
- Predict heart disease risk
- Identify high-risk combinations of health indicators
- Support preventative health screening and decision-making

---

## ğŸ§  Dataset

- **Source:** Kaggle â€“ Heart Disease Health Indicators (BRFSS 2015)
- **Format:** CSV
- **Size:** Large-scale public health survey data
- **Target Variable:** `HeartDiseaseorAttack`

Dataset link:  
https://www.kaggle.com/datasets/alexteboul/heart-disease-health-indicators-dataset

---

## ğŸ—ï¸ Architecture Overview

This project follows a **Data Lake (Schema-on-Read)** architecture on AWS:

Amazon S3 (Raw Data)
â†“
AWS Glue Crawler
â†“
AWS Glue Data Catalog
â†“
AWS Glue ETL Job
â†“
Amazon S3 (Processed Data)
â†“
Amazon EMR (Spark)
â†“
Amazon S3 (Final Outputs & Visualizations)


---

## â˜ï¸ AWS Services Used

- **Amazon S3** â€“ Data Lake storage
- **AWS Glue Crawler** â€“ Metadata discovery
- **AWS Glue ETL Job** â€“ Data transformation
- **AWS Glue Data Catalog** â€“ Centralized metadata repository
- **Amazon EMR** â€“ Distributed Spark processing
- **Apache Spark (PySpark)** â€“ Analytics & visualization
- **AWS IAM** â€“ Secure access control

---

## ğŸ“ Project Structure

AWS-heart-disease-data-pipeline/
â”œâ”€â”€ heart_disease_health_indicators_BRFSS2015.csv # Raw dataset
â”œâ”€â”€ heart_disease_glue_job.py # AWS Glue ETL script
â”œâ”€â”€ spark_script.py # Spark EDA & count plots
â”œâ”€â”€ sql_visualization.py # Spark SQL analytics
â”œâ”€â”€ images/ # Generated visualizations
â”œâ”€â”€ report.pdf # Final analytical report
â”œâ”€â”€ train.csv # ML training data
â”œâ”€â”€ test.csv # ML test data
â”œâ”€â”€ prediction_code.ipynb # Prediction notebook
â””â”€â”€ README.md

---

## ğŸ”„ Data Pipeline Workflow

### 1ï¸âƒ£ Data Ingestion
- Raw CSV uploaded to Amazon S3 (`finalproinput`)
- Schema applied at read-time (schema-on-read)

### 2ï¸âƒ£ Metadata Extraction
- AWS Glue Crawler scans S3 bucket
- Metadata stored in AWS Glue Data Catalog

### 3ï¸âƒ£ ETL Processing (AWS Glue)
- Data cleaned and transformed using PySpark
- Output written to S3 in compressed JSON format

### 4ï¸âƒ£ Big Data Processing (Amazon EMR)
- Spark jobs process transformed data
- Supports parallel and distributed execution

---

## ğŸ“Š Exploratory Data Analysis & Visualization

### Spark-Based Analysis (`spark_script.py`)
- Reads processed data from S3
- Converts Spark DataFrame â†’ Pandas DataFrame
- Generates:
  - Count plots
  - Box plots
- Uploads visualizations to S3

### Spark SQL Analytics (`sql_visualization.py`)
Performs SQL-based analysis for:
- **High-risk health combinations**
- **Healthy lifestyle impact**
- **Gender-based heart disease trends**

All plots are automatically saved and uploaded to S3.

---

## ğŸ“ˆ Sample Insights Generated

- Obesity, diabetes, and high cholesterol significantly increase heart disease risk
- Higher income + healthy diet + healthcare access correlates with lower risk
- Gender and age combinations reveal distinct risk patterns

---

## ğŸ§ª Predictive Modeling (Optional Extension)

- Data split into training and testing sets
- Prediction notebook (`prediction_code.ipynb`) explores ML-based risk prediction
- Enables future integration with scalable ML pipelines

---

## ğŸ¯ Key Learnings

- Designed a scalable **AWS-based data lake**
- Implemented **ETL using AWS Glue**
- Performed **distributed analytics using Spark**
- Built **automated visualization pipelines**
- Applied big data tools to real-world healthcare analytics

---

## ğŸš€ Future Enhancements

- Integrate AWS Athena for interactive querying
- Add ML models using Spark MLlib
- Automate pipeline with AWS Step Functions
- Deploy dashboards using Amazon QuickSight
